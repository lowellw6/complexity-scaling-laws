"""
Merge segments generated by gen_supervised_dataset.py into output_len datasets
"""

import os
import os.path as osp
import time
import numpy as np

import tsp
from tsp.utils import random_hex_str

from config.gen_supervised_dataset import args as config_args


### config
name = config_args["name"]
problem_size = config_args["problem_size"]
output_len = config_args["output_len"]
###


def matching_data_seg(file_str, psize):
    """True when file_str matches name (global) and psize pattern and length of segment is less than desired merged data length (global)"""
    file_str_segs = file_str.split("_")
    name_seg = file_str_segs[0]
    psize_seg = int(file_str_segs[1][:-1])  # -1 drops "n" trailing from string
    len_seg = int(file_str_segs[2][:-1])  # -1 drops "t" trailing from string
    return name_seg == name and psize_seg == psize and len_seg < output_len


def get_dataset_idx(data_file_names, psize):
    """Returns the latest index for a dataset matching psize, name (global), and output_len (global), assuming a trailing index"""
    max_val = -1
    for file_str in data_file_names:
        file_str_segs = file_str.split("_")
        name_seg = file_str_segs[0]
        psize_seg = int(file_str_segs[1][:-1])  # -1 drops "n" trailing from string
        len_seg = int(file_str_segs[2][:-1])  # -1 drops "t" trailing from string
        
        if name_seg == name and psize_seg == psize and len_seg == output_len:  # exact match
            idx_seg = file_str_segs[-1].split(".")[0]
            try:
                max_val = max(max_val, int(idx_seg))  # if not indexed, int() call will break
            except:
                print(f"ERROR unmerged segment '{file_str}' appears to match desired output size '{output_len}', leading to undefined behavior")
                quit()

    return max_val


problem_sizes = [problem_size] if type(problem_size) is int else problem_size
save_dir = osp.join(osp.dirname(osp.dirname(tsp.__file__)), "datasets")

for psize in problem_sizes:
    print(
        f"Merging solutions for '{name}' datasets of problem size {psize}..."
    )
    before = time.time()

    # gather/filter input files and next merged data index
    data_file_names = os.listdir(save_dir)
    selected_file_names = list(filter(lambda x: matching_data_seg(x, psize), data_file_names))

    next_idx = get_dataset_idx(data_file_names, psize) + 1

    # load and concatenate tensors
    data_arrs = [np.load(osp.join(save_dir, fn)) for fn in selected_file_names]
    merged_data = np.concatenate(data_arrs, axis=0)

    print(f"...done, {time.time() - before:.2f}s")

    # save one new dataset per output sized chunk (with any leftover in smaller remaining dataset)
    chunk_idxs = np.arange(output_len, len(merged_data), output_len)
    chunks = np.array_split(merged_data, chunk_idxs)

    for chunk in chunks:
        output_file_name = f"{name}_{psize}n_{len(chunk)}t_"
        
        if len(chunk) == output_len:
            output_file_name += f"{next_idx}.npy"
            next_idx += 1
        else:  # len(chunk) < output_len, so recycle into segments
            output_file_name += f"{random_hex_str(30)}.npy"

        save_path = osp.join(save_dir, output_file_name)

        with open(save_path, "wb") as f:
            np.save(f, chunk)

        print(f"Saved merged dataset @ {save_path}" if len(chunk) == output_len else f"Recycled remainder segement @ {save_path}")

    # delete loaded file names
    for merged_fn in selected_file_names:
        remove_path = osp.join(save_dir, merged_fn)
        os.remove(remove_path)

    print(f"Removed {len(selected_file_names)} {psize}n data segments after merging")
